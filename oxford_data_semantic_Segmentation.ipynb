{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú"
      ],
      "metadata": {
        "id": "zl-K5L-uQCdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-L5aJuEOs1O"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import OxfordIIITPet\n",
        "\n",
        "raw_dataset = OxfordIIITPet(\n",
        "    root=\"./data/oxfordpet\",\n",
        "    download=True,\n",
        "    target_types=\"segmentation\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7_hm0YhQKR3",
        "outputId": "93266bca-1e82-4b1f-9106-802f2b459f8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792M/792M [00:36<00:00, 21.8MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.2M/19.2M [00:01<00:00, 10.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overall Process Implementation"
      ],
      "metadata": {
        "id": "HWgj3hKRQU00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Compose\n",
        "import torchvision.transforms.functional as TF\n",
        "from pathlib import Path\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from PIL import Image\n",
        "from torch import Tensor\n",
        "from random import shuffle\n",
        "from torchvision.transforms import InterpolationMode\n"
      ],
      "metadata": {
        "id": "U6rloRORQ9Ge"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hnyoINRQNeVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model implementation"
      ],
      "metadata": {
        "id": "J4-yMjIUR5q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  \"\"\"(conv > bn > relu) * 2 \"\"\"\n",
        "  def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "    super().__init__()\n",
        "    if not mid_channels:\n",
        "      mid_channels = out_channels\n",
        "    self.double_conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(mid_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.maxpool_conv = nn.Sequential(\n",
        "        nn.MaxPool2d(2),\n",
        "        DoubleConv(in_channels, out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "    super().__init__()\n",
        "\n",
        "    if bilinear:\n",
        "      self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "      self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "    else:\n",
        "      self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
        "      self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    x1 = self.up(x1)\n",
        "    diffY = x2.size()[2] - x1.size()[2]\n",
        "    diffX = x2.size()[3] - x1.size()[3]\n",
        "    x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "    x = torch.cat([x2, x1], dim=1)\n",
        "    return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "LBcwXHZeTDe-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "    super(UNet, self).__init__()\n",
        "    self.n_channels = n_channels\n",
        "    self.n_classes = n_classes\n",
        "    self.bilinear = bilinear\n",
        "\n",
        "    self.inc = (DoubleConv(n_channels, 64))\n",
        "    self.down1 = (Down(64, 128))\n",
        "    self.down2 = (Down(128, 256))\n",
        "    self.down3 = (Down(256, 512))\n",
        "    factor = 2 if bilinear else 1\n",
        "    self.down4 = (Down(512, 1024 // factor))\n",
        "    self.up1 = (Up(1024, 512 // factor, bilinear))\n",
        "    self.up2 = (Up(512, 256 // factor, bilinear))\n",
        "    self.up3 = (Up(256, 128 // factor, bilinear))\n",
        "    self.up4 = (Up(128, 64, bilinear))\n",
        "    self.outc = (OutConv(64, n_classes))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.inc(x)\n",
        "    x2 = self.down1(x1)\n",
        "    x3 = self.down2(x2)\n",
        "    x4 = self.down3(x3)\n",
        "    x5 = self.down4(x4)\n",
        "    x = self.up1(x5, x4)\n",
        "    x = self.up2(x, x3)\n",
        "    x = self.up3(x, x2)\n",
        "    x = self.up4(x, x1)\n",
        "    logits = self.outc(x)\n",
        "    return logits\n",
        "\n",
        "  def use_checkpointing(self):\n",
        "    self.inc = torch.utils.checkpoint(self.inc)\n",
        "    self.down1 = torch.utils.checkpoint(self.down1)\n",
        "    self.down2 = torch.utils.checkpoint(self.down2)\n",
        "    self.down3 = torch.utils.checkpoint(self.down3)\n",
        "    self.down4 = torch.utils.checkpoint(self.down4)\n",
        "    self.up1 = torch.utils.checkpoint(self.up1)\n",
        "    self.up2 = torch.utils.checkpoint(self.up2)\n",
        "    self.up3 = torch.utils.checkpoint(self.up3)\n",
        "    self.up4 = torch.utils.checkpoint(self.up4)\n",
        "    self.outc = torch.utils.checkpoint(self.outc)"
      ],
      "metadata": {
        "id": "xxFeXupASB7Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset class (Single Transforms)"
      ],
      "metadata": {
        "id": "jGD_SX_paBqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "  \"\"\"Custom dataset \"\"\"\n",
        "  def __init__(self, images, masks, transforms):\n",
        "    self.images = images\n",
        "    self.masks = masks\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image_path = self.images[idx]\n",
        "    mask_path = self.masks[idx]\n",
        "\n",
        "    # open\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    mask = Image.open(mask_path).convert('L') # GRAYSCALE\n",
        "\n",
        "    # Apply transform\n",
        "    if self.transforms is not None:\n",
        "      image = self.transforms(image)\n",
        "      mask = self.transforms(mask)\n",
        "      # transforms.toTensor() Î•º ÌïòÍ≤åÎêòÎ©¥ image/255 Î•º Ìï¥ÏÑú Îã§Ïãú Í≥±Ìï¥Ï£ºÍ≥† -1 ÏùÑ Ìï¥Ï£ºÎäîÍ≤É.\n",
        "      mask = (mask*255).squeeze().to(torch.int64)\n",
        "      mask -= 1 # Ìï¥Îãπ Îç∞Ïù¥ÌÑ∞Í∞Ä 1,2,3 Î∂ÄÌÑ∞ Î†àÏù¥Î∏îÎßÅÏù¥ ÎêòÏñ¥ÏûàÎã§Í≥† Ìï®.\n",
        "\n",
        "    return image, mask"
      ],
      "metadata": {
        "id": "vf1KLaSiaBtF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset class (Multi Transforms)\n",
        "- random augment Îäî Í∞ôÏùÄ ÎûúÎç§Ïù¥ Îì§Ïñ¥Í∞ÄÏïºÌï¥ÏÑú self.transforms Î•º Í∞ÅÏûê Ï†ÅÏö©ÌïòÎ©¥ ÏïàÎê®.\n",
        "- RandomCrop : Ïù¥ÎØ∏ÏßÄ ÏûêÏ≤¥Î•º CRopping\n",
        "- Resize : Ï†ÑÏ≤¥ Ïù¥ÎØ∏ÏßÄÏùò Ìï¥ÏÉÅÎèÑÎ•º Î∞îÍøîÏ£ºÎäîÍ≤É( interpolation / nearest Î•º Ïç®ÏïºÌï®)"
      ],
      "metadata": {
        "id": "v2w1WvdLF5eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class JointRandomHorizontalFlip:\n",
        "  def __init__(self, p=0.5):\n",
        "    self.p = p\n",
        "\n",
        "  def __call__(self, img, mask):\n",
        "    if random.random() < self.p:\n",
        "      img = TF.hflip(img)\n",
        "      mask = TF.hflip(mask)\n",
        "\n",
        "    return img, mask\n",
        "\n",
        "class JointRandomCrop:\n",
        "  def __init__(self, size):\n",
        "    self.size = size\n",
        "\n",
        "  def __call__(self, img, mask):\n",
        "    i, j, h, w = transforms.RandomCrop.get_params(\n",
        "        img, output_size=self.size)\n",
        "    img = TF.crop(img, i, j, h, w)\n",
        "    mask = TF.crop(mask, i, j, h, w)\n",
        "\n",
        "    return img, mask\n",
        "\n",
        "class JointRandomRotate:\n",
        "  def __init__(self, degrees):\n",
        "    self.degrees = degrees\n",
        "\n",
        "  def __call__(self, img, mask):\n",
        "    angle = transforms.RandomRotation.get_params([-self.degrees, self.degrees])\n",
        "    img = TF.rotate(img, angle, interpolation=InterpolationMode.BILINEAR, fill=1)\n",
        "    mask = TF.rotate(mask, angle, interpolation=InterpolationMode.NEAREST, fill=1)\n",
        "\n",
        "    return img, mask\n",
        "\n",
        "class JointCompose:\n",
        "  def __init__(self, transforms_list):\n",
        "    self.transforms = transforms_list\n",
        "\n",
        "  def __call__(self, img, mask):\n",
        "\n",
        "    for t in self.transforms:\n",
        "      img, mask = t(img, mask)\n",
        "\n",
        "    return img, mask\n",
        "\n",
        "class SegmentationDataset_multi(Dataset):\n",
        "  def __init__(self, images, masks, crop_size, train=True):\n",
        "    self.images = images\n",
        "    self.masks = masks\n",
        "    self.image_size = (256, 256)\n",
        "\n",
        "    # Random Ïùò Í≤ΩÏö∞,\n",
        "    if train:\n",
        "      self.joint_transforms = JointCompose([\n",
        "          JointRandomHorizontalFlip(p=0.5),\n",
        "          # JointRandomRotate(degrees=10),\n",
        "          JointRandomCrop(crop_size)\n",
        "      ])\n",
        "      # Only Image,\n",
        "      self.image_transform = transforms.Compose([\n",
        "          # transforms.Resize(self.image_size, interpolation=InterpolationMode.BILINEAR),\n",
        "          transforms.ColorJitter(\n",
        "              brightness=0.2,\n",
        "              contrast=0.2,\n",
        "              saturation=0.2,\n",
        "              hue=0.2\n",
        "          ),\n",
        "          transforms.ToTensor(), # 0~255 Í∞Ä 0~1 Î°ú Î≥ÄÌôò\n",
        "          transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                  std=(0.229, 0.224, 0.225))\n",
        "          ])\n",
        "    else:\n",
        "      self.joint_transforms = None\n",
        "      # Only Image,\n",
        "      self.image_transform = transforms.Compose([\n",
        "          transforms.ToTensor(), # 0~255 Í∞Ä 0~1 Î°ú Î≥ÄÌôò\n",
        "          transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                  std=(0.229, 0.224, 0.225))\n",
        "          ])\n",
        "\n",
        "    # Only Mask,\n",
        "    # - interpolation : Nearest / float normalize Í∞ôÏùÄÍ±∞ ÌïòÎ©¥ ÏïàÎê®.\n",
        "    self.mask_transform = transforms.Compose([\n",
        "        # transforms.Resize(self.image_size, interpolation=InterpolationMode.NEAREST),\n",
        "        transforms.PILToTensor(), # Í∞í Ïú†ÏßÄ\n",
        "        transforms.Lambda(lambda x : x.long()-1) # oxfordpet Îç∞Ïù¥ÌÑ∞ mask Í∞Ä 1Î∂ÄÌÑ∞ ÏãúÏûëÌï¥ÏÑú\n",
        "    ])\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = Image.open(self.images[idx]).convert('RGB')\n",
        "    mask = Image.open(self.masks[idx])\n",
        "\n",
        "    # RESIZE\n",
        "    img = img.resize(self.image_size, resample=Image.BILINEAR)\n",
        "    mask = mask.resize(self.image_size, resample=Image.NEAREST)\n",
        "\n",
        "    img, mask = self.joint_transforms(img, mask)\n",
        "    img = self.image_transform(img)\n",
        "    mask = self.mask_transform(mask)\n",
        "    mask = mask.squeeze()\n",
        "\n",
        "    return img, mask"
      ],
      "metadata": {
        "id": "QbvlzqPkF5hT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qmsOGdW5F5kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train loop"
      ],
      "metadata": {
        "id": "-UsMyCWUSDvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPKIeNuLRm7u",
        "outputId": "60385bdc-f6ce-466c-9ab2-a658cfbba734"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "bilinear = True\n",
        "classes = 3\n",
        "image_size = (256, 256)\n",
        "\n",
        "model = UNet(n_channels=3, n_classes=classes, bilinear=bilinear)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f'Network:\\n'\n",
        "      f'\\t{model.n_channels} input channels\\n'\n",
        "      f'\\t{model.n_classes} output channels (classes)\\n'\n",
        "      f'\\t{\"Bilinear\" if model.bilinear else \"Transposed conv\"} upscaling')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxrGc-yEX5lZ",
        "outputId": "dd3f8029-4815-4420-c895-7daf65bbd43c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network:\n",
            "\t3 input channels\n",
            "\t3 output channels (classes)\n",
            "\tBilinear upscaling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Dataset\n",
        "image_path = \"/content/data/oxfordpet/oxford-iiit-pet/images\"\n",
        "mask_path = \"/content/data/oxfordpet/oxford-iiit-pet/annotations/trimaps\"\n",
        "split_rate = 0.2\n",
        "\n",
        "# Create a list of image paths\n",
        "img_paths = sorted([\n",
        "    os.path.join(image_path, name)\n",
        "    for name in os.listdir(image_path)\n",
        "    if name.endswith('.jpg')\n",
        "])\n",
        "\n",
        "# Create a list of mask paths\n",
        "mask_paths = sorted([\n",
        "    os.path.join(mask_path, name)\n",
        "    for name in os.listdir(mask_path)\n",
        "    if not name.startswith('.') and name.endswith('.png')\n",
        "])\n",
        "\n",
        "tmp = list(zip(img_paths, mask_paths))\n",
        "shuffle(tmp)\n",
        "img_paths, mask_paths = zip(*tmp)\n",
        "img_paths, mask_paths = list(img_paths), list(mask_paths)\n",
        "train_imgs = img_paths[int(split_rate*len(img_paths)):]\n",
        "train_masks = mask_paths[int(split_rate*len(mask_paths)):]\n",
        "test_imgs = img_paths[:int(split_rate * len(img_paths))]\n",
        "test_masks = mask_paths[:int(split_rate * len(mask_paths))]\n",
        "\n",
        "# SINGLE TRANSFORMS\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(image_size),\n",
        "#     transforms.ToTensor()\n",
        "# ])\n",
        "\n",
        "# train_dataset = SegmentationDataset(train_imgs, train_masks, transform)\n",
        "# test_dataset = SegmentationDataset(test_imgs, test_masks, transform)\n",
        "\n",
        "# Multi Transforms\n",
        "train_dataset = SegmentationDataset_multi(train_imgs, train_masks, crop_size=image_size)\n",
        "test_dataset = SegmentationDataset_multi(test_imgs, test_masks, crop_size=image_size)\n",
        "print('Train images: {}\\n Test images: {}'.format(len(train_dataset), len(test_dataset)))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "           num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "           num_workers=2, pin_memory=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=30, eta_min=1e-6\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtN5OVjpZcHm",
        "outputId": "5f2442f6-f4ca-4b95-d0cf-2fdf3ef400bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 5912\n",
            " Test images: 1478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dice loss (reference)\n",
        "ÏûÖÎ†•ÏúºÎ°ú softmax / one hot Î™®Îëê Í±∞Ï≥êÏÑú ÎÑ£Ïñ¥Ï§òÏïºÌï®."
      ],
      "metadata": {
        "id": "TUPLFXUzTZ7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
        "    # Average of Dice coefficient for all batches, or for a single mask\n",
        "    assert input.size() == target.size()\n",
        "    assert input.dim() == 3 or not reduce_batch_first\n",
        "\n",
        "    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n",
        "\n",
        "    inter = 2 * (input * target).sum(dim=sum_dim)\n",
        "    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n",
        "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
        "\n",
        "    dice = (inter + epsilon) / (sets_sum + epsilon)\n",
        "    return dice.mean()\n",
        "\n",
        "\n",
        "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
        "    # Average of Dice coefficient for all classes\n",
        "    # input / target: (B, C, H, W)\n",
        "    # flatten : (B*C, H, W) Î°ú Î≥ÄÌôò\n",
        "    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n",
        "\n",
        "\n",
        "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
        "    # Dice loss (objective to minimize) between 0 and 1\n",
        "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
        "    return 1 - fn(input, target, reduce_batch_first=True)"
      ],
      "metadata": {
        "id": "IR0pzVj0iUut"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dice loss (from chatgpt)\n",
        "ÏïÑÎûò Ìï®Ïàò ÏïàÏóêÏÑú one hot / softmax Ï≤òÎ¶¨ Îã§ Ìï¥Ï§å."
      ],
      "metadata": {
        "id": "ONLJv2UOTdNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coeff_binary(prob, target, eps):\n",
        "  # prob : after sigmoid, shape = (B, 1, H, W) or (B, H, W)\n",
        "  if prob.dim() == 4:\n",
        "    prob = prob.squeeze(1) # (B, H, W)\n",
        "  if target.dim() == 4:\n",
        "    target = target.squeeze(1)\n",
        "\n",
        "  prob = prob.contiguous().view(prob.size(0), -1) # (B, H*W) Shape ÏúºÎ°ú Î≥ÄÌôò\n",
        "  target = target.contiguous().view(target.size(0), -1)\n",
        "\n",
        "  inter = (prob * target).sum(dim=1)\n",
        "  union = prob.sum(dim=1) + target.sum(dim=1)\n",
        "  dice = (2*inter+eps) / (union+eps)\n",
        "  return dice.mean()\n",
        "\n",
        "\n",
        "def dice_coeff_multiclass(prob, target, eps=1e-6):\n",
        "  # prob: (B, C, H, W)\n",
        "  # Target: (B, H, W)\n",
        "  num_classes = prob.shape[1]\n",
        "  target_one_hot = F.one_hot(target, num_classes) # (B, H, W, C)\n",
        "  target_one_hot = target_one_hot.permute(0, 3, 1, 2) # (B, C, H, W)\n",
        "  target_one_hot = target_one_hot.float()\n",
        "\n",
        "  dices = []\n",
        "  for c in range(num_classes):\n",
        "    dices.append(dice_coeff_binary(prob[:, c], target_one_hot[:, c], eps=eps))\n",
        "\n",
        "  return torch.stack(dices).mean()\n",
        "\n",
        "def dice_loss_multiclass(logits: torch.Tensor, target: torch.Tensor):\n",
        "  prob = torch.softmax(logits, dim=1)\n",
        "  return 1 - dice_coeff_multiclass(prob, target)"
      ],
      "metadata": {
        "id": "lssZPDUzTdPW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric (mIOU)"
      ],
      "metadata": {
        "id": "SMekbP0Feypo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_miou(pred, target, num_classes, ignore_index=None, eps=1e-6):\n",
        "  \"\"\"\n",
        "  pred : (B, H, W)\n",
        "  target: (B, H, W)\n",
        "  \"\"\"\n",
        "\n",
        "  assert pred.shape == target.shape\n",
        "\n",
        "  if ignore_index is not None:\n",
        "    mask = target != ignore_index\n",
        "    pred  = pred[mask]\n",
        "    target = target[mask]\n",
        "\n",
        "  ious = []\n",
        "  for c in range(num_classes):\n",
        "    pred_c = (pred == c)\n",
        "    target_c = (target == c)\n",
        "    inter = (pred_c & target_c).sum().float() # ÍµêÏßëÌï©\n",
        "    union = pred_c.sum().float() + target_c.sum().float() - inter # Ìï©ÏßëÌï©\n",
        "\n",
        "    if union == 0:\n",
        "      continue\n",
        "    ious.append((inter+eps) / (union+eps))\n",
        "\n",
        "  if len(ious) == 0:\n",
        "    return torch.tensor(0.0)\n",
        "  return torch.stack(ious).mean()"
      ],
      "metadata": {
        "id": "9juzDXYpeysd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = float('inf')\n",
        "checkpoint_path = \"/content/segmentation_checkpoints\"\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "num_classes = model.n_classes\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  train_miou_total = 0.0\n",
        "  train_batches = 0\n",
        "  test_loss = 0.0\n",
        "  test_miou_total = 0.0\n",
        "  test_batches = 0\n",
        "\n",
        "  # for i, (images, masks) in tqdm( enumerate(train_loader)):\n",
        "  for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n",
        "    images, masks = images.to(device), masks.to(device)\n",
        "    pred = model(images)\n",
        "    # CE : pred (B, C, H, W) raw logits, target=(B, H, W) long() [0,1,2 class] ÏùÑ Í∏∞ÎåÄ\n",
        "    loss = criterion(pred, masks)\n",
        "    # dice_loss_ = dice_loss(\n",
        "    #     F.softmax(pred, dim=1).float(),\n",
        "    #     F.one_hot(masks, model.n_classes).permute(0, 3, 1, 2).float(),\n",
        "    #     multiclass=True\n",
        "    #   )\n",
        "\n",
        "    dice_loss_ = dice_loss_multiclass(pred, masks)\n",
        "    loss += dice_loss_\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      preds = torch.argmax(pred, dim=1)\n",
        "      miou = compute_miou(preds, masks, num_classes)\n",
        "      train_miou_total += miou.item()\n",
        "      train_batches += 1\n",
        "\n",
        "  avg_train_loss = train_loss / len(train_loader)\n",
        "  avg_train_miou = train_miou_total / train_batches\n",
        "\n",
        "  # evaluate\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    # for i, (images, masks) in tqdm(enumerate(test_loader)):\n",
        "    for images, masks in tqdm(test_loader, desc=f\"Epoch {epoch} [Eval]\"):\n",
        "      images, masks = images.to(device), masks.to(device)\n",
        "      pred = model(images)\n",
        "      loss = criterion(pred, masks)\n",
        "      # loss += dice_loss(\n",
        "      #   F.softmax(pred, dim=1).float(),\n",
        "      #   F.one_hot(masks, model.n_classes).permute(0, 3, 1, 2).float(),\n",
        "      #   multiclass=True\n",
        "      # )\n",
        "      dice_loss_ = dice_loss_multiclass(pred, masks)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      preds = torch.argmax(pred, dim=1)\n",
        "      miou = compute_miou(preds, masks, num_classes=num_classes)\n",
        "      test_miou_total += miou.item()\n",
        "      test_batches += 1\n",
        "\n",
        "    # Calculate\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    avg_test_miou = test_miou_total / test_batches\n",
        "\n",
        "    print(f\"[Epoch {epoch}] \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, Train mIoU: {avg_train_miou:.4f} | \"\n",
        "          f\"Test Loss: {avg_test_loss:.4f}, Test mIoU: {avg_test_miou:.4f}\")\n",
        "\n",
        "\n",
        "  scheduler.step()\n",
        "  # Save checkpoint\n",
        "  ckpt_file = os.path.join(checkpoint_path, \"best_model.pth\")\n",
        "  if avg_test_loss < best_loss:\n",
        "    best_loss = avg_test_loss\n",
        "    torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'loss': best_loss\n",
        "      }, ckpt_file)\n",
        "    print(f\"üìå Best model saved at epoch {epoch} (loss={best_loss:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yigB8sc4ZtmM",
        "outputId": "d9c13559-9583-4980-9133-d3bcffee6055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]:   4%|‚ñç         | 14/370 [00:09<03:43,  1.59it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "AcxtroRpd8vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# class index ‚Üí RGB Ïª¨Îü¨\n",
        "colors = {\n",
        "    0: (0, 0, 0),       # background\n",
        "    1: (255, 0, 0),     # class1\n",
        "    2: (0, 255, 0),     # class2\n",
        "}\n",
        "\n",
        "def decode_segmap(mask):\n",
        "    h,w = mask.shape\n",
        "    result = np.zeros((h,w,3), dtype=np.uint8)\n",
        "    for cls, color in colors.items():\n",
        "        result[mask == cls] = color\n",
        "    return result\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=1,num_workers=8, pin_memory=True)\n",
        "\n",
        "model = UNet(n_channels=3, n_classes=3, bilinear=bilinear).to(device)\n",
        "state_dict = torch.load(\"/content/segmentation_checkpoints/best_model.pth\", map_location=device)\n",
        "model.load_state_dict(state_dict['model_state_dict'])\n",
        "\n",
        "save_dir = \"/content/seg_results/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  for i, (images, masks) in tqdm(enumerate(test_loader)):\n",
        "    images = images.to(device)\n",
        "    pred = model(images) # (1, 3, H, W)\n",
        "    pred_mask = torch.argmax(pred, dim=1) # (1, H, W)\n",
        "    mask_np = pred_mask.squeeze().cpu().numpy().astype(np.uint8)\n",
        "    # ÌååÏùº Ïù¥Î¶Ñ Ï†ÄÏû•\n",
        "    save_path = os.path.join(save_dir, f\"mask_{i}.png\")\n",
        "    color_mask = decode_segmap(mask_np)\n",
        "    cv2.imwrite(save_path, color_mask)\n",
        "\n",
        "print(f\"\\nüìå Saved inference masks to: {save_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "OhW1AUAAegF6",
        "outputId": "29860191-e783-4349-a402-27ba6e8db5ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "647it [00:15, 40.83it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2443359772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# ÌååÏùº Ïù¥Î¶Ñ Ï†ÄÏû•\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"mask_{i}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mcolor_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_segmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2443359772.py\u001b[0m in \u001b[0;36mdecode_segmap\u001b[0;34m(mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bLOzzPmLdbb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Í∏∞ÌÉÄ"
      ],
      "metadata": {
        "id": "toe0ItY20-KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Image normalize (mean, std) Í≥ÑÏÇ∞ Î∞©Î≤ï ###\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_mean_std(loader):\n",
        "    mean = 0.\n",
        "    std = 0.\n",
        "    total = 0\n",
        "\n",
        "    for images, _ in tqdm(loader):\n",
        "        batch_samples = images.size(0)\n",
        "        images = images.view(batch_samples, images.size(1), -1)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std  += images.std(2).sum(0)\n",
        "        total += batch_samples\n",
        "\n",
        "    mean /= total\n",
        "    std  /= total\n",
        "    return mean, std\n",
        "\n",
        "dataset = datasets.ImageFolder(\"train_images\",\n",
        "    transform=transforms.ToTensor())\n",
        "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "mean, std = compute_mean_std(loader)\n",
        "print(mean, std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Fz9KT-PmK1u_",
        "outputId": "28ea6ab4-01c5-4a3d-e008-ece89e7d4b1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train_images'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2319091802.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m dataset = datasets.ImageFolder(\"train_images\",\n\u001b[0m\u001b[1;32m     25\u001b[0m     transform=transforms.ToTensor())\n\u001b[1;32m     26\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_images'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-0u9vEZjK6Ko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}