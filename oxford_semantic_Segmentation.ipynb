{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú"
      ],
      "metadata": {
        "id": "zl-K5L-uQCdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-L5aJuEOs1O"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import OxfordIIITPet\n",
        "\n",
        "raw_dataset = OxfordIIITPet(\n",
        "    root=\"./data/oxfordpet\",\n",
        "    download=True,\n",
        "    target_types=\"segmentation\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7_hm0YhQKR3",
        "outputId": "6d75ec68-c0b7-4148-c76b-cfddbfa9bdcc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792M/792M [00:50<00:00, 15.7MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.2M/19.2M [00:02<00:00, 7.86MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overall Process Implementation"
      ],
      "metadata": {
        "id": "HWgj3hKRQU00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from pathlib import Path\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from PIL import Image\n",
        "from torch import Tensor\n",
        "from torchvision.transforms import Compose"
      ],
      "metadata": {
        "id": "U6rloRORQ9Ge"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model implementation"
      ],
      "metadata": {
        "id": "J4-yMjIUR5q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  \"\"\"(conv > bn > relu) * 2 \"\"\"\n",
        "  def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "    super().__init__()\n",
        "    if not mid_channels:\n",
        "      mid_channels = out_channels\n",
        "    self.double_conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(mid_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.maxpool_conv = nn.Sequential(\n",
        "        nn.MaxPool2d(2),\n",
        "        DoubleConv(in_channels, out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "    super().__init__()\n",
        "\n",
        "    if bilinear:\n",
        "      self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "      self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "    else:\n",
        "      self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
        "      self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    x1 = self.up(x1)\n",
        "    diffY = x2.size()[2] - x1.size()[2]\n",
        "    diffX = x2.size()[3] - x1.size()[3]\n",
        "    x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "    x = torch.cat([x2, x1], dim=1)\n",
        "    return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "LBcwXHZeTDe-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "    super(UNet, self).__init__()\n",
        "    self.n_channels = n_channels\n",
        "    self.n_classes = n_classes\n",
        "    self.bilinear = bilinear\n",
        "\n",
        "    self.inc = (DoubleConv(n_channels, 64))\n",
        "    self.down1 = (Down(64, 128))\n",
        "    self.down2 = (Down(128, 256))\n",
        "    self.down3 = (Down(256, 512))\n",
        "    factor = 2 if bilinear else 1\n",
        "    self.down4 = (Down(512, 1024 // factor))\n",
        "    self.up1 = (Up(1024, 512 // factor, bilinear))\n",
        "    self.up2 = (Up(512, 256 // factor, bilinear))\n",
        "    self.up3 = (Up(256, 128 // factor, bilinear))\n",
        "    self.up4 = (Up(128, 64, bilinear))\n",
        "    self.outc = (OutConv(64, n_classes))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.inc(x)\n",
        "    x2 = self.down1(x1)\n",
        "    x3 = self.down2(x2)\n",
        "    x4 = self.down3(x3)\n",
        "    x5 = self.down4(x4)\n",
        "    x = self.up1(x5, x4)\n",
        "    x = self.up2(x, x3)\n",
        "    x = self.up3(x, x2)\n",
        "    x = self.up4(x, x1)\n",
        "    logits = self.outc(x)\n",
        "    return logits\n",
        "\n",
        "  def use_checkpointing(self):\n",
        "    self.inc = torch.utils.checkpoint(self.inc)\n",
        "    self.down1 = torch.utils.checkpoint(self.down1)\n",
        "    self.down2 = torch.utils.checkpoint(self.down2)\n",
        "    self.down3 = torch.utils.checkpoint(self.down3)\n",
        "    self.down4 = torch.utils.checkpoint(self.down4)\n",
        "    self.up1 = torch.utils.checkpoint(self.up1)\n",
        "    self.up2 = torch.utils.checkpoint(self.up2)\n",
        "    self.up3 = torch.utils.checkpoint(self.up3)\n",
        "    self.up4 = torch.utils.checkpoint(self.up4)\n",
        "    self.outc = torch.utils.checkpoint(self.outc)"
      ],
      "metadata": {
        "id": "xxFeXupASB7Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset class"
      ],
      "metadata": {
        "id": "jGD_SX_paBqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "  \"\"\"Custom dataset \"\"\"\n",
        "  def __init__(self, images, masks, transforms):\n",
        "    self.images = images\n",
        "    self.masks = masks\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image_path = self.images[idx]\n",
        "    mask_path = self.masks[idx]\n",
        "\n",
        "    # open\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    mask = Image.open(mask_path).convert('L') # GRAYSCALE\n",
        "\n",
        "    # Apply transform\n",
        "    if self.transforms is not None:\n",
        "      image = self.transforms(image)\n",
        "      mask = self.transforms(mask)\n",
        "      # transforms.toTensor() Î•º ÌïòÍ≤åÎêòÎ©¥ image/255 Î•º Ìï¥ÏÑú Îã§Ïãú Í≥±Ìï¥Ï£ºÍ≥† -1 ÏùÑ Ìï¥Ï£ºÎäîÍ≤É.\n",
        "      mask = (mask*255).squeeze().to(torch.int64)\n",
        "      mask -= 1 # Ìï¥Îãπ Îç∞Ïù¥ÌÑ∞Í∞Ä 1,2,3 Î∂ÄÌÑ∞ Î†àÏù¥Î∏îÎßÅÏù¥ ÎêòÏñ¥ÏûàÎã§Í≥† Ìï®.\n",
        "\n",
        "    return image, mask"
      ],
      "metadata": {
        "id": "vf1KLaSiaBtF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train loop"
      ],
      "metadata": {
        "id": "-UsMyCWUSDvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPKIeNuLRm7u",
        "outputId": "de188b73-9284-4815-9561-db3a442c5b8c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "bilinear = True\n",
        "classes = 3\n",
        "image_size = (256, 256)\n",
        "\n",
        "model = UNet(n_channels=3, n_classes=classes, bilinear=bilinear)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f'Network:\\n'\n",
        "      f'\\t{model.n_channels} input channels\\n'\n",
        "      f'\\t{model.n_classes} output channels (classes)\\n'\n",
        "      f'\\t{\"Bilinear\" if model.bilinear else \"Transposed conv\"} upscaling')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxrGc-yEX5lZ",
        "outputId": "4cd265e9-baa5-4587-dac8-1343dd9f931a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network:\n",
            "\t3 input channels\n",
            "\t3 output channels (classes)\n",
            "\tBilinear upscaling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "## Dataset\n",
        "image_path = \"/content/data/oxfordpet/oxford-iiit-pet/images\"\n",
        "mask_path = \"/content/data/oxfordpet/oxford-iiit-pet/annotations/trimaps\"\n",
        "split_rate = 0.2\n",
        "\n",
        "# Create a list of image paths\n",
        "img_paths = sorted([\n",
        "    os.path.join(image_path, name)\n",
        "    for name in os.listdir(image_path)\n",
        "    if name.endswith('.jpg')\n",
        "])\n",
        "\n",
        "# Create a list of mask paths\n",
        "mask_paths = sorted([\n",
        "    os.path.join(mask_path, name)\n",
        "    for name in os.listdir(mask_path)\n",
        "    if not name.startswith('.') and name.endswith('.png')\n",
        "])\n",
        "\n",
        "tmp = list(zip(img_paths, mask_paths))\n",
        "shuffle(tmp)\n",
        "img_paths, mask_paths = zip(*tmp)\n",
        "img_paths, mask_paths = list(img_paths), list(mask_paths)\n",
        "train_imgs = img_paths[int(split_rate*len(img_paths)):]\n",
        "train_masks = mask_paths[int(split_rate*len(mask_paths)):]\n",
        "test_imgs = img_paths[:int(split_rate * len(img_paths))]\n",
        "test_masks = mask_paths[:int(split_rate * len(mask_paths))]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = SegmentationDataset(train_imgs, train_masks, transform)\n",
        "test_dataset = SegmentationDataset(test_imgs, test_masks, transform)\n",
        "print('Train images: {}\\n Test images: {}'.format(len(train_dataset), len(test_dataset)))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "           num_workers=8, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "           num_workers=8, pin_memory=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=30, eta_min=1e-6\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtN5OVjpZcHm",
        "outputId": "3f4a700f-68d8-472d-e7aa-ce0497f7d715"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 5912\n",
            " Test images: 1478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
        "    # Average of Dice coefficient for all batches, or for a single mask\n",
        "    assert input.size() == target.size()\n",
        "    assert input.dim() == 3 or not reduce_batch_first\n",
        "\n",
        "    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n",
        "\n",
        "    inter = 2 * (input * target).sum(dim=sum_dim)\n",
        "    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n",
        "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
        "\n",
        "    dice = (inter + epsilon) / (sets_sum + epsilon)\n",
        "    return dice.mean()\n",
        "\n",
        "\n",
        "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
        "    # Average of Dice coefficient for all classes\n",
        "    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n",
        "\n",
        "\n",
        "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
        "    # Dice loss (objective to minimize) between 0 and 1\n",
        "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
        "    return 1 - fn(input, target, reduce_batch_first=True)"
      ],
      "metadata": {
        "id": "IR0pzVj0iUut"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = float('inf')\n",
        "checkpoint_path = \"/content/segmentation_checkpoints\"\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  test_loss = 0.0\n",
        "\n",
        "  # for i, (images, masks) in tqdm( enumerate(train_loader)):\n",
        "  for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n",
        "    images, masks = images.to(device), masks.to(device)\n",
        "    pred = model(images)\n",
        "    loss = criterion(pred, masks)\n",
        "    dice_loss_ = dice_loss(\n",
        "        F.softmax(pred, dim=1).float(),\n",
        "        F.one_hot(masks, model.n_classes).permute(0, 3, 1, 2).float(),\n",
        "        multiclass=True\n",
        "      )\n",
        "    loss += dice_loss_\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # evaluate\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    # for i, (images, masks) in tqdm(enumerate(test_loader)):\n",
        "    for images, masks in tqdm(test_loader, desc=f\"Epoch {epoch} [Eval]\"):\n",
        "      images, masks = images.to(device), masks.to(device)\n",
        "      pred = model(images)\n",
        "      loss = criterion(pred, masks)\n",
        "      loss += dice_loss(\n",
        "        F.softmax(pred, dim=1).float(),\n",
        "        F.one_hot(masks, model.n_classes).permute(0, 3, 1, 2).float(),\n",
        "        multiclass=True\n",
        "      )\n",
        "      test_loss += loss.item()\n",
        "\n",
        "    # Calculate\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    print('Train loss: {} Test loss: {}'.format(avg_train_loss, avg_test_loss))\n",
        "\n",
        "  scheduler.step()\n",
        "  # Save checkpoint\n",
        "  ckpt_file = os.path.join(checkpoint_path, \"best_model.pth\")\n",
        "  if avg_test_loss < best_loss:\n",
        "    best_loss = avg_test_loss\n",
        "    torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'loss': best_loss\n",
        "      }, ckpt_file)\n",
        "    print(f\"üìå Best model saved at epoch {epoch} (loss={best_loss:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yigB8sc4ZtmM",
        "outputId": "ee1c075a-78df-4a60-a2d4-3c061452c0c3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "370it [03:57,  1.56it/s]\n",
            "93it [00:20,  4.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.8864724131854804 Test loss: 0.6911396659830565\n",
            "üìå Best model saved at epoch 1 (loss=0.6911)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "370it [03:56,  1.56it/s]\n",
            "93it [00:21,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6077948816724725 Test loss: 0.5773407079840219\n",
            "üìå Best model saved at epoch 2 (loss=0.5773)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "370it [03:56,  1.56it/s]\n",
            "93it [00:20,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.5287298710765065 Test loss: 0.5385000455764032\n",
            "üìå Best model saved at epoch 3 (loss=0.5385)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "370it [03:57,  1.56it/s]\n",
            "93it [00:20,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.47995712378540556 Test loss: 0.5138944602140816\n",
            "üìå Best model saved at epoch 4 (loss=0.5139)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "370it [03:56,  1.56it/s]\n",
            "93it [00:20,  4.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.443558214645128 Test loss: 0.5053440204230688\n",
            "üìå Best model saved at epoch 5 (loss=0.5053)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "AcxtroRpd8vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# class index ‚Üí RGB Ïª¨Îü¨\n",
        "colors = {\n",
        "    0: (0, 0, 0),       # background\n",
        "    1: (255, 0, 0),     # class1\n",
        "    2: (0, 255, 0),     # class2\n",
        "}\n",
        "\n",
        "def decode_segmap(mask):\n",
        "    h,w = mask.shape\n",
        "    result = np.zeros((h,w,3), dtype=np.uint8)\n",
        "    for cls, color in colors.items():\n",
        "        result[mask == cls] = color\n",
        "    return result\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=1,num_workers=8, pin_memory=True)\n",
        "\n",
        "model = UNet(n_channels=3, n_classes=3, bilinear=bilinear).to(device)\n",
        "state_dict = torch.load(\"/content/best_model.pth\", map_location=device)\n",
        "model.load_state_dict(state_dict['model_state_dict'])\n",
        "\n",
        "save_dir = \"/content/seg_results/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  for i, (images, masks) in tqdm(enumerate(test_loader)):\n",
        "    images = images.to(device)\n",
        "    pred = model(images) # (1, 3, H, W)\n",
        "    pred_mask = torch.argmax(pred, dim=1) # (1, H, W)\n",
        "    mask_np = pred_mask.squeeze().cpu().numpy().astype(np.uint8)\n",
        "    # ÌååÏùº Ïù¥Î¶Ñ Ï†ÄÏû•\n",
        "    save_path = os.path.join(save_dir, f\"mask_{i}.png\")\n",
        "    color_mask = decode_segmap(mask_np)\n",
        "    cv2.imwrite(save_path, color_mask)\n",
        "\n",
        "print(f\"\\nüìå Saved inference masks to: {save_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhW1AUAAegF6",
        "outputId": "ce721369-608b-44d4-a0b8-884dc38005df"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1478it [00:31, 46.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìå Saved inference masks to: /content/seg_results/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1foTaV_eiyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}